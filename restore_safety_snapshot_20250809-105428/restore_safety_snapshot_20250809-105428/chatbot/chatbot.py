import os
import re
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
import json
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cache para armazenar a cadeia de QA
qa_chain_cache = None

def format_docs(docs):
    """Fun√ß√£o auxiliar para formatar os documentos recuperados em uma √∫nica string."""
    return "\n\n".join(doc.page_content for doc in docs)

def inicializar_chatbot():
    """
    Carrega o √≠ndice FAISS e inicializa a cadeia de QA usando LCEL,
    configurada para retornar a resposta e os documentos de origem.
    """
    global qa_chain_cache
    try:
        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            logger.error("‚ùå Erro: A chave de API do Google (GOOGLE_API_KEY) n√£o foi definida.")
            return False
        
        logger.info("üîë Configurando API do Google...")
        genai.configure(api_key=api_key)

        caminho_indice = os.path.join(os.path.dirname(__file__), "..", "web_app", "faiss_index_estruturado")

        if not os.path.isdir(caminho_indice):
            logger.error(f"‚ùå Erro: O diret√≥rio do √≠ndice '{caminho_indice}' n√£o foi encontrado.")
            return False

        logger.info("üîç Carregando √≠ndice FAISS...")
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vectorstore = FAISS.load_local(caminho_indice, embeddings, allow_dangerous_deserialization=True)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 8})

        logger.info("ü§ñ Criando a cadeia de QA com LCEL para retornar fontes...")
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.02, streaming=True)
        
        prompt_template = """Voc√™ √© um assistente especializado em artigos t√©cnicos do Grupo Botic√°rio para orienta√ß√£o de franqueados.
        Analise TODA a pergunta do usu√°rio e CONSOLIDE informa√ß√µes similares para otimizar TMA/TME.
        
        **REGRAS CR√çTICAS PARA CONSOLIDA√á√ÉO:**
        1. ELIMINE repeti√ß√µes - agrupe informa√ß√µes similares em uma √∫nica entrada
        2. PRIORIZE informa√ß√µes mais relevantes - m√°ximo 5 causas principais
        3. CONSOLIDE procedimentos similares - evite duplica√ß√µes
        4. USE hierarquia: Principais ‚Üí Secund√°rias ‚Üí T√©cnicas
        5. AGRUPE por categoria: Hardware, Software, Configura√ß√£o, Rede
        6. SEJA CONCISO - foque no essencial para reduzir tempo de leitura
        7. UNIFIQUE terminologia - use termos consistentes
        
        **FORMATO OBRIGAT√ìRIO - Layout Azul Profissional:**
        
        ## 1. C√≥digo do Artigo: 
        [Extraia qualquer c√≥digo num√©rico encontrado no contexto relacionado √† pergunta]
        
        ## 2. T√≠tulo do Artigo: 
        [Extraia o t√≠tulo mais relevante do contexto, mesmo que parcial]
        
        ## 3. Descri√ß√£o das Principais Causas:
        
        **üîß HARDWARE (F√≠sicas)**
        **‚Ä¢ Causa 1: [Nome Consolidado]**
           - Descri√ß√£o: [Agrupe problemas f√≠sicos similares]
           - Refer√™ncia: [Artigos principais]
        
        **üíª SOFTWARE (Sistema)**  
        **‚Ä¢ Causa 2: [Nome Consolidado]**
           - Descri√ß√£o: [Agrupe problemas de software similares]
           - Refer√™ncia: [Artigos principais]
        
        **‚öôÔ∏è CONFIGURA√á√ÉO (Setup)**
        **‚Ä¢ Causa 3: [Nome Consolidado]**
           - Descri√ß√£o: [Agrupe problemas de configura√ß√£o similares]  
           - Refer√™ncia: [Artigos principais]
        
        **üåê CONECTIVIDADE (Rede)**
        **‚Ä¢ Causa 4: [Nome Consolidado]**
           - Descri√ß√£o: [Agrupe problemas de rede similares]
           - Refer√™ncia: [Artigos principais]
        
        *(M√ÅXIMO 5 CAUSAS CONSOLIDADAS - Elimine duplica√ß√µes e agrupe similares)*
        
        ## 4. Solu√ß√µes Consolidadas por Prioridade:
        
        ### **üöÄ SOLU√á√ÉO R√ÅPIDA (1¬∫ Tentar)**
        1. **Verifica√ß√£o Inicial:** [Consolidar verifica√ß√µes b√°sicas similares]
        2. **A√ß√£o Imediata:** [Consolidar solu√ß√µes r√°pidas similares]
        
        ### **üîß SOLU√á√ÉO T√âCNICA (2¬∫ N√≠vel)**  
        1. **Diagn√≥stico:** [Consolidar procedimentos de diagn√≥stico]
        2. **Configura√ß√£o:** [Consolidar ajustes t√©cnicos]
        3. **Teste:** [Consolidar valida√ß√µes]
        
        ### **üõ†Ô∏è SOLU√á√ÉO AVAN√áADA (3¬∫ N√≠vel)**
        1. **Manuten√ß√£o:** [Consolidar procedimentos de manuten√ß√£o]
        2. **Substitui√ß√£o:** [Consolidar processos de substitui√ß√£o]
        
        ## 5. Preven√ß√£o:
        **üìã CHECKLIST DE PREVEN√á√ÉO:**
        - [Consolidar a√ß√µes preventivas similares]
        - [Agrupar verifica√ß√µes peri√≥dicas]
        
        **CONTEXTO DISPON√çVEL:**
        {context}
        
        **PERGUNTA DO USU√ÅRIO:**
        {question}
        
        **RESPOSTA CONSOLIDADA:**"""

        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=prompt_template
        )

        # Define a cadeia RAG que processa os documentos
        rag_chain_from_docs = (
            RunnablePassthrough.assign(context=(lambda x: format_docs(x["source_documents"])))
            | prompt
            | llm
            | StrOutputParser()
        )

        # Define a cadeia final que recupera os documentos e depois chama a cadeia acima
        qa_chain_cache = RunnableParallel(
            {
                "source_documents": retriever,
                "question": RunnablePassthrough()
            }
        ).assign(answer=rag_chain_from_docs)
        
        logger.info("‚úÖ Chatbot inicializado com sucesso para streaming!")
        return True

    except Exception as e:
        logger.error(f"‚ùå Ocorreu um erro durante a inicializa√ß√£o do chatbot: {e}")
        return False

def get_chatbot_answer_stream(question):
    """
    Recebe uma pergunta e retorna um gerador para a resposta e as fontes.
    """
    if qa_chain_cache is None:
        logger.error("‚ùå O chatbot n√£o foi inicializado corretamente.")
        yield "data: " + json.dumps({"error": "O chatbot n√£o foi inicializado corretamente." }) + "\n\n"
        return

    try:
        logger.info(f"üîç Processando pergunta: {question[:100]}...")
        stream = qa_chain_cache.stream(question)
        
        # O primeiro chunk pode conter as fontes e/ou o in√≠cio da resposta
        first_chunk = next(stream)
        
        source_docs = first_chunk.get("source_documents", [])
        unique_sources = []
        seen_sources = set()
        for doc in source_docs:
            source_file = doc.metadata.get('source_file', 'Origem desconhecida')
            if source_file not in seen_sources:
                unique_sources.append({
                    "title": doc.metadata.get('article_title', 'N/A'),
                    "source_file": source_file
                })
                seen_sources.add(source_file)
        
        logger.info(f"üìö Encontradas {len(unique_sources)} fontes √∫nicas")
        
        # Envia as fontes primeiro
        yield "data: " + json.dumps({"sources": unique_sources}) + "\n\n"

        # Envia o primeiro peda√ßo da resposta, se houver
        if 'answer' in first_chunk:
            yield "data: " + json.dumps({"token": first_chunk['answer']}) + "\n\n"

        # Continua enviando o resto da resposta em streaming
        for chunk in stream:
            if 'answer' in chunk:
                yield "data: " + json.dumps({"token": chunk['answer']}) + "\n\n"

    except Exception as e:
        error_message = f"Ocorreu um erro ao processar a pergunta: {e}"
        logger.error(f"‚ùå {error_message}")
        yield "data: " + json.dumps({"error": error_message}) + "\n\n"

# Bloco de teste atualizado
if __name__ == '__main__':
    if inicializar_chatbot():
        print("\n--- Chatbot de Conhecimento (Teste Local com Fontes) ---")
        print("Digite 'sair' para terminar.")
        while True:
            pergunta = input("\nSua pergunta: ")
            if pergunta.lower() == 'sair':
                break
            
            # A fun√ß√£o de teste agora usa o stream
            full_response = ""
            sources_received = []
            for chunk_data in get_chatbot_answer_stream(pergunta):
                try:
                    data = json.loads(chunk_data.replace("data: ", "").strip())
                    if "token" in data:
                        full_response += data["token"]
                    if "sources" in data:
                        sources_received = data["sources"]
                except json.JSONDecodeError:
                    print(f"Erro ao decodificar JSON: {chunk_data}")
                    continue
            
            if full_response or sources_received:
                print("\n--- Resposta Gerada ---")
                print(full_response)
                print("\n--- Fontes ---")
                if sources_received:
                    for source in sources_received:
                        print(f"- T√≠tulo: {source['title']}, Arquivo: {source['source_file']}")
                else:
                    print("Nenhuma fonte encontrada.")
                print("---------------------")
            else:
                print("Erro: Nenhuma resposta ou fonte recebida.")